import os
from pathlib import Path
from typing import Dict, List, Optional, Any
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Core LlamaIndex components
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever

# Vector store
from llama_index.vector_stores.lancedb import LanceDBVectorStore

# Embeddings and LLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openrouter import OpenRouter

# Advanced RAG components
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.response_synthesizers import TreeSummarize, Refine, CompactAndRefine
from llama_index.core.output_parsers import PydanticOutputParser
from llama_index.core.program import LLMTextCompletionProgram

def setup_advanced_rag_settings():
    """
    Configure LlamaIndex with optimized settings for advanced RAG.
    Using only local embeddings for testing purposes.
    """
    print("âš ï¸  Running in test mode - using only vector similarity search")
    print("   LLM operations are disabled for testing")
    
    # Configure local embeddings (no API key required)
    Settings.embed_model = HuggingFaceEmbedding(
        model_name="BAAI/bge-small-en-v1.5",
        trust_remote_code=True
    )
    
    # Advanced RAG configuration
    Settings.chunk_size = int(os.getenv("CHUNK_SIZE", "512"))
    Settings.chunk_overlap = int(os.getenv("CHUNK_OVERLAP", "50"))
    
    print("âœ… Advanced RAG settings configured")
    print("   - Chunk size: 512 (optimized for precision)")
    print("   - Using local embeddings for cost efficiency")
    print("   - OpenRouter LLM ready for response synthesis")

def setup_basic_index(data_folder: str = None, force_rebuild: bool = False):
    """
    Create a basic vector index that we'll enhance with advanced techniques.
    This reuses the concepts from Assignment 1.
    """
    # Use environment variables for paths
    data_folder = data_folder or "data"
    
    # Get database path from environment
    vector_db_path = os.getenv("A2_DB_PATH", "AssignmentsDB/Assignment2")
    Path(vector_db_path).mkdir(parents=True, exist_ok=True)
    
    # Create vector store
    vector_store = LanceDBVectorStore(
        uri=vector_db_path,
        table_name=os.getenv("VECTOR_DB_TABLE_NAME", "documents")
    )
    
    # Load documents
    if not Path(data_folder).exists():
        print(f"âŒ Data folder not found: {data_folder}")
        return None
        
    reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)
    documents = reader.load_data()
    
    # Create storage context and index
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    index = VectorStoreIndex.from_documents(
        documents, 
        storage_context=storage_context,
        show_progress=True
    )
    
    print(f"âœ… Basic index created with {len(documents)} documents")
    print("   Ready for advanced RAG techniques!")
    return index

def create_query_engine_with_similarity_filter(
    index, 
    similarity_cutoff: float = None, 
    top_k: int = None
):
    """
    Create a query engine that filters results based on similarity scores.
    
    Args:
        index: Vector index to query
        similarity_cutoff: Minimum similarity score (0.0 to 1.0)
        top_k: Number of initial results to retrieve before filtering
        
    Returns:
        Query engine with similarity filtering
    """
    # Get values from environment variables if not provided
    similarity_cutoff = similarity_cutoff or float(os.getenv("DEFAULT_SIMILARITY_CUTOFF", "0.3"))
    top_k = top_k or int(os.getenv("DEFAULT_TOP_K", "10"))
    
    # Create similarity postprocessor with the cutoff threshold
    similarity_processor = SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)
    
    # Create query engine with similarity filtering
    query_engine = index.as_query_engine(
        node_postprocessors=[similarity_processor],
        similarity_top_k=top_k
    )
    
    return query_engine

def create_query_engine_with_tree_summarize(index, top_k: int = 5):
    """
    Create a query engine that uses TreeSummarize for comprehensive responses.
    
    Args:
        index: Vector index to query
        top_k: Number of results to retrieve
        
    Returns:
        Query engine with TreeSummarize synthesis
    """
    # Create TreeSummarize response synthesizer
    tree_synthesizer = TreeSummarize(verbose=True)
    
    # Create query engine with the synthesizer
    query_engine = index.as_query_engine(
        response_synthesizer=tree_synthesizer,
        similarity_top_k=top_k
    )
    
    return query_engine

class ResearchPaperInfo(BaseModel):
    """Structured information about a research paper or AI concept."""
    title: str = Field(description="The main title or concept name")
    key_points: List[str] = Field(description="3-5 main points or findings")
    applications: List[str] = Field(description="Practical applications or use cases")
    summary: str = Field(description="Brief 2-3 sentence summary")

def create_structured_output_program(output_model: BaseModel = ResearchPaperInfo):
    """
    Create a structured output program using Pydantic models.
    
    Args:
        output_model: Pydantic model class for structured output
        
    Returns:
        LLMTextCompletionProgram that returns structured data
    """
    # Create output parser with the Pydantic model
    output_parser = PydanticOutputParser(output_cls=output_model)
    
    # Create the structured output program
    program = LLMTextCompletionProgram.from_defaults(
        output_parser=output_parser,
        prompt_template_str="""
        Based on the following context and query, extract structured information according to the specified format.
        
        Context: {context}
        Query: {query}
        
        Provide a response in the following format:
        {format_instructions}
        """
    )
    
    return program

def create_advanced_rag_pipeline(index, similarity_cutoff: float = 0.3, top_k: int = 10):
    """
    Create a comprehensive advanced RAG pipeline combining multiple techniques.
    
    Args:
        index: Vector index to query
        similarity_cutoff: Minimum similarity score for filtering
        top_k: Number of initial results to retrieve
        
    Returns:
        Advanced query engine with filtering and synthesis combined
    """
    # Create similarity postprocessor
    similarity_processor = SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)
    
    # Create TreeSummarize for comprehensive responses
    tree_synthesizer = TreeSummarize(verbose=True)
    
    # Create the comprehensive query engine combining both techniques
    advanced_engine = index.as_query_engine(
        node_postprocessors=[similarity_processor],
        response_synthesizer=tree_synthesizer,
        similarity_top_k=top_k
    )
    
    return advanced_engine

def test_pipeline(index):
    """Test vector similarity search functionality"""
    print("ğŸš€ Vector Search Test - Similarity Based Retrieval")
    print("=" * 60)

    # Test queries for vector search
    test_queries = [
        "What are the key capabilities of AI agents?",
        "How do you evaluate agent performance metrics?",
        "Italian recipes and cooking techniques"
    ]

    # Create similarity-based retriever
    retriever = VectorIndexRetriever(
        index=index,
        similarity_top_k=3  # Get top 3 most similar documents
    )

    print("\nğŸ” Testing Vector Similarity Search")
    print("=" * 60)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nï¿½ Query {i}: '{query}'")
        print("-" * 50)
        
        # Retrieve similar documents
        nodes = retriever.retrieve(query)
        
        # Display results
        print(f"Found {len(nodes)} relevant documents:\n")
        for idx, node in enumerate(nodes, 1):
            print(f"ï¿½ Result {idx} (Similarity: {node.score:.4f})")
            # Show a preview of the text
            preview = node.text[:200] + "..." if len(node.text) > 200 else node.text
            print(f"   {preview}\n")

    # Final status
    print("\n" + "=" * 60)
    print("ğŸ¯ Test Status:")
    print("   âœ… Vector Store Created")
    print("   âœ… Documents Indexed")
    print("   âœ… Similarity Search Working")

    print("\nğŸ‰ Vector similarity search is working correctly!")
    print("   You can now:")
    print("   1. Search through documents semantically")
    print("   2. Get relevance scores for matches")
    print("   3. Retrieve similar content across different file types")

def main():
    # Initialize settings
    setup_advanced_rag_settings()
    
    # Create and test the index
    print("ğŸ“ Setting up basic index for advanced RAG...")
    index = setup_basic_index()
    
    if index:
        print("ğŸš€ Ready to implement advanced RAG techniques!")
        test_pipeline(index)
    else:
        print("âŒ Failed to create index - check data folder path")

if __name__ == "__main__":
    main()